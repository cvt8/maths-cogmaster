\documentclass[french,12pt,a4paper]{book}
\usepackage[a4paper,left=1.5 cm,right=1.5cm,top=1.8cm,bottom=1.8cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fourier}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs} 
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{authblk}
\usepackage{url}
\usepackage{stmaryrd}

\newcommand{\SN}[2]{[\! [{#1},{#2}]\! ]}% segment d'entiers
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\U}{\mathbb{U}}
\newcommand{\Proba}{\mathbb{P}}
\newcommand{\transp}[1]{#1^{\small T}}
\newcommand{\ch}{\mathop{\mathrm{ch}}\nolimits}
\newcommand{\sh}{\mathop{\mathrm{sh}}\nolimits}
\def\tn#1{\left|\left|\left|#1\right|\right|\right|}

\newtheorem{theo}{Theorem}[chapter]
\newtheorem{exo}{Exercise}[chapter]
\newtheorem{defin}{Definition}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{prop}{Proposition}[chapter]
\newtheorem{cor}{Corollar}[chapter]
\newtheorem{rem}{Remark}[chapter]
\newtheorem{hyp}{Assumption}[chapter]
\numberwithin{equation}{chapter}

\usepackage{enumitem}

%\newtheorem{exo}{Exercice}%[section]

\title{Cogmaster maths club - lesson}
\date{\today}
\author{Constantin Vaillant-Tenzer}
\affil{École Normale Supérieure - PSL, Université de Paris \\ constantin.tenzer@ens.psl.eu}
%\affil{Sorbonne Université}
%\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\begin{document}

\maketitle

\tableofcontents

\chapter{Preliminary}

\chapter{Real analysis}

Let $U$ be an (open) subset of $\R$ and $f$ an application from $U$ to $\R$. 

 The correction of the exercises in this chapter is available on request to Constantin Vaillant-Tenzer. 

\section{Some general definitions}

\begin{defin}
We say that $f$ is a \textbf{increasing function} if for all $(x,y) \in U^2$ such that $x < y$, $f(x) \leq f(y)$

We say that $f$ is a \textbf{strictly increasing function} if for all $(x,y) \in U^2$ such that $x < y$, $f(x) < f(y)$

We say that $f$ is a\textbf{ decreasing function} if for all $(x,y) \in U^2$ such that $x < y$, $f(x) \geq f(y)$

We say that $f$ is a \textbf{strictly decreasing function }if for all $(x,y) \in U^2$ such that $x < y$, $f(x) > f(y)$

We say that $f$ is a \textbf{monotonous} if $f$ is increasing or decreasing. 

We say that $f$ is a \textbf{strictly monotonous} if $f$ is strictly increasing or strictly decreasing. 
\end{defin}

\begin{rem}
If $f$ in increasing, $-f$ is decreasing and reciprocally. 
\end{rem}

\begin{defin}[Interval]
Let $I \subset \R$, $a = \inf I$ and $b = \sup I$.  We say that $I$ is an \textbf{interval} if for all $c$ such that $a < c < b$, $c \in I$.

We say that the interval is \textbf{closed }if $a \in I$ and $b \in I$.

We say that the interval is \textbf{open }if $a \notin I$ and $b \notin I$.
\end{defin}

\section{Limits}

\begin{defin}
We say that a sequence $(u_n)_{n \in \N}$ tends to some \textbf{limit} $l \in \C$ if :
$$\forall \varepsilon>0\quad\exists n_0 \in \N \quad\forall n \geq n_0\quad|{x_n - l}| < \varepsilon $$
We note it : $u_n \underset{n \rightarrow + \infty}{ \longrightarrow} l$.

We say that a sequence $(u_n)_{n \in \N}$ tends to $+ \infty$ if : 
$$ \forall M\in\R\quad\exists n_0 \in \N \quad\forall n \geq n_0\quad x_n > M $$

We say that a sequence $(u_n)_{n \in \N}$ tends to $- \infty$ if : 
$$\forall M\in\R\quad\exists n_0 \in \N\quad\forall n \geq n_0\quad x_n < M$$
\end{defin}

\begin{defin}
We say that $f(x)$ tends to $l$ when $x$ tends to $a$ if :
$$\quad\forall \varepsilon \in \R_+^*\quad\exists \delta \in \R_+^*\quad\forall x \in \mathcal D_f\quad(|{x - a}| < \delta \Rightarrow |f(x)-L|<\varepsilon)$$
We note it : $f(x) \underset{x \rightarrow +a}{ \longrightarrow} l$.

We can have, similarly than for the sequences, the definitions of the limits in infinite or for infinite values. 
\end{defin}

\begin{exo}
Write the missing definitions of a limit.
\end{exo}

\begin{prop}[Unity of the limit]
If the limit exists, it is unique. 
\end{prop}

\begin{proof}
We will do this demonstration within the frame of sequences, with finite limits. The demonstration would be similar for the other cases. 

Let be $l, l' \in \C$. Let assume that $u_n \underset{n \rightarrow + \infty}{ \longrightarrow} l$ and that $u_n \underset{n \rightarrow + \infty}{ \longrightarrow} l'$. 

Let be $\varepsilon > 0$. Hence, we have $n_0$ and $n_1$ such that : 
$$ \forall n \geq n_0, \ |u_n - l| \leq \varepsilon$$
and 
$$ \forall n \geq n_1, \ |u_n - l'| \leq \varepsilon$$
Let be $N = \max (n_0, n_1)$. We hence have : 

$|u_N - l| \leq \varepsilon$ and $|u_N - l'| \leq \varepsilon$. Hence :
$$| u_N -l - u_N + l'| \leq 2 \varepsilon$$
Hence, 
$$| l - l'| \leq 2 \varepsilon$$
And so for all $\varepsilon > 0$. Hence, $l = l'$.
\end{proof}

Therefore it is possible to use the \textbf{lim} notation. For instance : $\lim \limits_{n \rightarrow + \infty} u_n = l$.

\begin{rem}
The limits is linearly compoundable :
$$\forall (\lambda_i)_{I \in I} \in \R^{I \subset \N}, \ \lim \limits_{x \rightarrow a} \sum_{i \in I} \lambda_i f_i(x) = \sum_{i \in I} \lambda_i \lim \limits_{x \rightarrow a}  f_i(x)$$
Provided that the limit in $a$ of the functions $(f_i)$ exists. 
\end{rem}

\begin{rem}
Sometimes a sequence can have no limit. For instance the sequence $(\sin (n))_{n \in \N}$
\end{rem}

\begin{exo}[Cèsaro lemma - basic version]
Let $(u_n)_{n \in \N} \in \R^\N$ a sequence that converges to some limit $l~\in~\C$.

Prove that $\dfrac{1}{N} \sum \limits_{n=1}^N u_n \underset{N \rightarrow + \infty} \rightarrow l$.
\end{exo}
\emph{Hint :} Assume first that $l = 0$ and use the limit definition.


\section{Continuity}

\begin{defin}
We say that $f$ is c\textbf{ontinuous in a point} $a \in U$ if :
$$\lim \limits_{x \rightarrow a, \ x > a} f(x) = \lim \limits_{x \rightarrow a, \  x < a} f(x)  = \lim \limits_{x \rightarrow a } f(x) = f(a)$$
\end{defin}

\begin{defin}[Continuity on a set]
We say that a function $f$ is \textbf{continuous} in a set $E \subset U$ if $f$ is continuous in all points of $E$. 
\end{defin}

\begin{rem}
This notion is also linear. 
\end{rem}

\begin{theo}[Intermediate Values theorem (IVT)]
Let assume that $f$ is a continuous function and $I$ an interval. Then $f(I)$ is an interval. 
\end{theo}

\begin{proof}
We shall prove the first case, $f(a)<u<f(b)$. The second case is similar.

Let $S$ be the set of all $x\in[a,b]$ such that $f(x)\leq u$. Then $S$ is non-empty since $a$ is an element of $S$. Since $S$ is non-empty and bounded above by $b$, by completeness, the supremum $c=\sup S$ exists. That is, $c$ is the smallest number that is greater than or equal to every member of $S$. We claim that $f(c)=u$.

Fix some $\varepsilon > 0$. Since $f$ is continuous, there is a $\delta>0$ such that $|f(x) - f(c)| < \varepsilon$ whenever $|x-c| < \delta$. This means that
$$f(x)-\varepsilon<f(c)<f(x)+\varepsilon$$
for all $x\in(c-\delta,c+\delta)$. By the properties of the supremum, there exists some $a^*\in (c-\delta,c]$ that is contained in $S$, and so
$$f(c)<f(a^*)+\varepsilon\le u+\varepsilon .$$
Picking $a^{**}\in(c,c+\delta)$, we know that $a^{**}\not\in S$ because $c$ is the supremum of $S$. This means that
$$f(c)>f(a^{**})-\varepsilon\ > u-\varepsilon .$$
Both inequalities
$$u-\varepsilon<f(c)< u+\varepsilon$$
are valid for all $\varepsilon>0$, from which we deduce $f(c)=u$ as the only possible value, as stated.
\end{proof}

\begin{exo}
\label{exoTVI}
Let be :
\begin{align*}
f : \R &\rightarrow \R \\
x &\longmapsto 2x^2 + 5x -1
\end{align*}
\begin{enumerate}
\item Justify that $f$ is well defined and continuous on $\R$ ;
\item Compute $f(0$ and $f(1)$ ;
\item Prove that it exists some $x_0 \in [0,1]$ such that $f(x_0) = 0$.
\end{enumerate}
\end{exo}

\begin{cor}[Bijection theorem]
If $f$ is continuous and strictly monotonous, then on all interval $]a,b[$, with $a < b$ on $\overline{\R}$, then, for all value $x$ comprised between $f(a)$ and $f(b)$, it exits a unique value $c \in ]a, b[$ such that $f(c) = x$. 
\end{cor}

\begin{proof}
Let assume that $f$ is strictly increasing (the demonstration is symmetrical if $f$ strictly decreasing). 

Since $f$ is strictly growing $\inf f(]a,b[) = f(a)$ and $\sup  f(]a,b[) = f(b)$. Hence, since $f$ is continuous and so respect the intermediate values properties, $f(]a,b[) = ]f(a), f(b)[$. Let be some $x \in ]f(a), f(b)[$. Through the IVT, it exists some $c \in ]a, b[$ such that $f(c) = x$. 

This $c$ is unique since, if there would be another $c'$ such that $f(c') = x$, we would have $f(c') < x$ or  $f(c') > x$ through the strict monotony.
\end{proof}

\begin{prop}
The image of a closed interval by a continuous function is a closed interval.
\end{prop}

\begin{proof}
Through the ITV, The image of a closed interval $K$ by a continuous function is an interval $f(K)$. 

Let be $m$ and $M$ the respectively inferior and superior bounds of $f(K)$. They are either in $f(K)$ or at its border. However, this corresponds therefore to the borders of $K$, that are included in $K$. Hence, we have the closure.

\end{proof}

\section{Derivation}

\subsection{Definitions and basic properties}

\begin{defin}[Derivative on a point]
We say that a function $f$ is \textbf{derivable} in $a \in U$ if the number
$$\lim_{h \rightarrow 0} \dfrac{f(a +h ) - f(a)}{h}$$
exists and is finite. If so, this number is called the \textbf{derivative of $f$ in $a$} and is noted $f'(a)$.
\end{defin}

\begin{defin}[Derivative on a set]
We say that a function $f$ is \textbf{derivable} in a set $E \subset U$ if $f$ is derivable in all points of $E$. We can therefore define the derivative function.
\end{defin}


\begin{prop}
Let $a \in U$. If $f$ is derivable in $a$, then $f$ is continuous in $a$. 
\end{prop}

\begin{proof}

\end{proof}

\begin{prop}[Linearity of the derivative]
Let be $\lambda \in R$, $f$ and $g$ two derivable function defined respectively on the subsets $U$ and $V$ of $\R$. Then, for all $a \in U \cap V$, 
$$(f+ \lambda g)'(a) = f'(a) + \lambda g'(a).$$
\end{prop}

\begin{proof}
Trivial. You just need to write the definition, using the linearity properties of limits.
\end{proof}

\begin{exo}
\label{exoDERIV}
Let be :
\begin{align*}
f : \R &\rightarrow \R \\
x &\longmapsto 2x^2 + 5x -1
\end{align*}
\begin{enumerate}
\item Justify that $f$ is derivable on $\R$ ;
\item Plot the variation tab ;
\item Is there only only $x_0$ such as defined is the question 3 of the exercise~\ref{exoTVI} ?
\end{enumerate}
\end{exo}

\subsection{Some derivatives to know}

\begin{defin}[The exponential function]
The exponential function is the only function $f \in \R^\R$ such that $f$ is derivable and :
\begin{itemize}
\item $f'(x) = f(x)$
\item $f(0) = 1$
\end{itemize} 
This function is noted $x \longmapsto \exp (x)$ or $e^x$.
\end{defin}


\begin{defin}[Neperian logarithm]
The Neperian logarithm is the reciprocal function of the exponential. It is noted $x \longmapsto \ln (x)$.
\end{defin}

\section{Integrals}

\subsection{Integration on a segment}

\subsection{Integration on an interval}

\subsection{Integration of measurable functions}

\section{Series}

\chapter{Differential equations}

\section{First order ordinary differential equations}
Let be $D$ an open set of $\R$.

\subsection{Generalities}

\begin{defin}
Let $f$ be a derivable function. A first order linear differential equation is an equation of the form, for all $x \in D$:
$$a(x) f'(x) + b(x)f(x) + c(x) = 0$$
\end{defin}

Or general (non linear) version is, for all $x \in D$:
$$a(x)f'(x) +  b(x)g(f(x)) + c(x) = 0$$

\begin{rem}
For the subsets of $D$ where $a$ don't get equal to 0, we can write differential equations under the form:
$$f'(x) + b(x)f(x) + c(x) = 0$$
and generally:
$$f'(x) +  b(x)g(f(x)) + c(x) = 0$$
\end{rem}

\begin{theo}[Cauchy - Lipschitz]
With the given initial condition $f(x_0) = f_{x_0}$, and assuming that $g$ is a locally lipschitz function, it exists and there is a unique solution $f$ to the differential equations.
\end{theo}

\begin{proof}
We will demonstrate the theorem for the case of linear differential equations, as it also gives a general method to solve those equations.

The set of solutions to (EH) on \( J \) is included in the set of functions from \( J \) to \( \mathbb{R} \) or \( \mathbb{C} \). Moreover, it is non-empty since the zero function is a solution to (EH) on \( J \). Finally, it is clear that if \( f \) and \( g \) are solutions to (EH) on \( J \), and \( \lambda \) and \( \mu \) are real or complex scalars, then \( (\lambda f + \mu g) \) is also a solution to (EH) on \( J \), showing that this set is stable under linear combinations and thus forms a vector space over \( K \).

On an interval \( J \) contained in \( I \) where \( a \) does not vanish, \( y \) is a solution to (EH) if and only if:
\[ \forall t \in J, a(t)y' + b(t)y = c(t) \]

Since the function \( t \mapsto \frac{1}{a(t)} \) is defined and continuous on \( J \), it has primitives on \( J \). Multiplying the equality on \( y \) by \( \int dt \) from \( t_a \) to \( t_b \), it is equivalent to:
\[ \forall t \in J, \int_{t_a}^{t_b} \frac{1}{a(t)}b(t) dt + C = \int_{t_a}^{t_b} \frac{1}{a(t)}c(t) dt \]

This can be rewritten as:
\[ \forall t \in J, 0 = \int_{t_a}^{t_b} \frac{1}{a(t)}[c(t) - b(t)y] dt \]

Or alternatively:
\[ \forall t \in J, 0 = \exp\left(\int_{t_a}^{t_b} \frac{1}{a(t)}dt\right)[c(t) - b(t)y] \]

Now, on an interval, a real or complex-valued function has a zero derivative if and only if it is constant. In other words:
\[ (y \text{ is a solution to } (EH) \text{ on } J) \Leftrightarrow (\exists k \in \mathbb{R} \text{ or } \mathbb{C}, \forall t \in J, \int_{t_a}^{t_b} \frac{1}{a(t)}b(t)y = k) \]

By denoting, for example:
\[ \forall t \in J, y_0(t) = \exp\left(-\int_{t_0}^{t} \frac{du}{a(u)}\right)\left(\int_{t_0}^{t} \frac{\exp\left(\int_{t_0}^{u} \frac{dv}{a(v)}\right)c(v)}{a(u)} du + C_1\right) \]

Where \( t_0 \in J \), it is clear that any solution to (EH) on \( J \) is proportional to \( y_0 \). Furthermore, since \( y_0 \) is also a solution to (EH) on \( J \) and the set \( SJ(EH) \) of these solutions forms a vector space over \( K \), we can conclude that \( SJ(EH) = \text{Vect}(y_0) \).

Finally, since this function is non-zero, \( SJ(EH) \) is indeed of dimension 1.

Let \( y \) be a function defined, continuous, and differentiable on \( J \) included in \( I \), and let \( y_0 \) be a particular solution of (E) on \( J \). Then \( y \) is a solution to (E) on \( J \) if and only if:
\[ \forall t \in J, a(t)y'(t) + b(t)y(t) = c(t) = a(t)y_0'(t) + b(t)y_0(t) \]

This is also equivalent to:
\[ \forall t \in J, a(t)[y - y_0]'(t) + b(t)[y - y_0](t) = 0 \]

Or finally equivalent to:
\[ (y - y_0) \in SJ(EH) \text{, or } (\exists y_H \in SJ(EH), y = y_0 + y_H) \]

Finally, if \( a \) does not vanish on \( J \), the solutions to (EH) on \( J \), forming a vector line, and the solutions to (E) on \( J \), given the previous form, indeed form an affine line or an affine space of dimension 1.
\end{proof}

\begin{exo}
\begin{enumerate}
\item For the following equations, after determining the solution, plot the solution curve by showcasing the values and slopes of interest (initial value, final value, initial slope):
\begin{itemize}
    \item $y' - 4y = 0$ with $y(t = 2) = 9$
    \item $y' + 7y = 9$ with $y(t = 0) = 0$
\end{itemize}

Let $y(t)$ be a quantity satisfying the differential equation:
\[ \dot{y} + y \cdot \tau = 0, \text{ with } y(t = 0) = y_0 \]

\item  We seek a solution of the form: $y = e^{rt}$. Reinject this function into the equation. Deduce $r$. Reinject this function into the initial condition. Deduce $\tau$. Plot the solution function $y(t)$.

Let $y(t)$ be a quantity satisfying the differential equation:
\[ \dot{y} + y \cdot \tau = \alpha, \text{ with } y(t = 0) = 0 \]

\item We seek a solution of the form: $y = e^{rt} + b$. Reinject this function into the equation. By identifying the functions dependent on time and the constants, deduce $r$ and $b$. Reinject this function into the initial condition. Deduce $\tau$. Plot the solution function $y(t)$.
\end{enumerate}
\end{exo}

\subsection{Homogeneous differential equation}

To solve a differential equation, one need first to solve its homegenious associated equation:
\begin{equation}
f'(x) +  b(x)g(f(x)) =0
\end{equation}

This can be done, by integrating on the two sides the differential equation:

\begin{prop}
The set of solutions to the homogeneous linear equation $a(t)x' + b(t)x = 0$, 
over the interval \(I\), with a certain \(t_0\) in \(I\) such that \(x(t_0) = x_0\), is defined for all \(t \in I\) by
\[x(t) = x_0 e^{F(t)},\]
where $F(t) = - \int_{t_0}^{t} \frac{b(s)}{a(s)} \, ds$.
\end{prop}

\begin{exo}
Integrate the equation $\frac{dv}{dt} = bv^2$ with $v(t = 0) = v_0$. Plot the curve of $v(t)$.
\end{exo}

\textbf{Solution:} We separate the variables, which gives $\frac{dv}{v^2} = bdt$. Now, we know that $\frac{dv}{v^2} = \frac{1}{v}dv$, so the integration gives $\frac{1}{v} = bt + \text{cste}$ (we can verify this by differentiating this equality to confirm that it satisfies the differential equation).

To determine the integration constant, we use the initial condition: $v(t = 0) = v_0$. Therefore, we evaluate the equality $\frac{1}{v} = bt + \text{cste}$ at $t = 0$, which gives $\frac{1}{v_0} = b \cdot 0 + \text{cste}$. Hence, $\text{cste} = \frac{1}{v_0}$.

Thus, $\frac{1}{v} = bt + \frac{1}{v_0}$, which finally gives $v = \frac{v_0}{1 + btv_0}$, and one can easily verify its homogeneity.

\subsection{Differential equation with second members}

The solution is the sum of :
\begin{enumerate}
\item A particular solution $f P (t)$ to be determined. This solution does not depend on the initial conditions and
can be fully determined without initial conditions. It can be shown to be of the same form as the second member.
as the second member: if the second member is a sinusoidal function, a particular solution of the type
type solution. To determine the particular solution, we assume that it is of the same form as the second member.
as the second member, keeping as much generality as possible. This general form is then reinfected into
the differential equation under study to determine any undetermined parameters;
\item And the solution $e^{kt}$ of the associated homogeneous linear equation.
\end{enumerate}

\begin{prop}
The general solution of the equation $a(t)x' + b(t)x = d(t)$,
over the interval \(I\) with a certain \(t_0\) in \(I\) such that \(x(t_0) = x_0\) is given by:
\[x(t) = \exp \left( - \int_{t_0}^{t} \frac{b(s)}{a(s)} \, ds \right) \left( x_0 + \int_{t_0}^{t} \frac{d(s)}{a(s)} \exp \left( \int_{s}^{t_0} \frac{b(\sigma)}{a(\sigma)} \, d\sigma \right) \, ds \right).\]
\end{prop}


\begin{rem}
The Cauchy theorem gives a general method for solving first order differential equations. This method is called the variation of the constant method:


Consider the first-order linear ordinary differential equation:
\[ \frac{dy}{dt} + p(t)y = g(t) \]

Let find the Integrating Factor \( \mu(t) \)
\[ \mu(t) = e^{- \int p(t) dt} \]

Then, multiply Both Sides by the Integrating Factor
\[ \mu(t) \frac{dy}{dt} + p(t)\mu(t)y = \mu(t)g(t) \]

Step 3: Recognize the Left-Hand Side as the Derivative of \( \mu(t)y \)
\[ \frac{d}{dt}(\mu(t)y) = \mu(t)\frac{dy}{dt} + \frac{d\mu(t)}{dt}y \]

Step 4: Integrate Both Sides
\[ \int \frac{d}{dt}(\mu(t)y) dt = \int \mu(t)g(t) dt \]

This leads to:
\[ \mu(t)y = \int \mu(t)g(t) dt + C \]
where \( C \) is the constant of integration.

Step 5: Solve for \( y(t) \)
\[ y(t) = \frac{1}{\mu(t)} \left( \int \mu(t)g(t) dt + C \right) \]
\end{rem}

\begin{exo}
Solve the following differential equations on the relevant subset of $\R$:
\begin{enumerate}
\item $f'(x) + xf(x) = 0$
\item $f'(x) + \pi f(x) = u_0$
\item $f'(x) + 5f = \sin(x)$
\end{enumerate}
\end{exo}


\section{Euler Method}

The Euler method provides a straightforward approach to solving ODEs numerically. While it has limitations, such as sensitivity to step size, it serves as a foundation method in the study of numerical solutions to differential equations.

\subsection{Basic Idea}

The Euler method is a numerical technique for approximating the solution to an ODE. It is based on the idea of discretizing the independent variable \( t \) into small intervals and approximating the derivative with a finite difference. The basic update formula for the Euler method is:

\[ y_{n+1} = y_n + h \cdot f(t_n, y_n) \]

where \( y_n \) is the approximation of \( y \) at time \( t_n \), \( h \) is the step size, and \( f(t_n, y_n) \) is the derivative at \( (t_n, y_n) \).

\subsection{Algorithm}

The Euler method algorithm can be summarized as follows:

\begin{enumerate}
    \item Choose the initial condition \( y_0 \) and the step size \( h \).
    \item Iterate using the update formula: \( y_{n+1} = y_n + h \cdot f(t_n, y_n) \).
    \item Repeat until the desired endpoint is reached.
\end{enumerate}

\subsection{Error}

The error of the Euler's method is proportional to $h^2$, by considering Taylor expension of the function. 

\end{document}